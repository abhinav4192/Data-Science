{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isfile, join\n",
    "import logging\n",
    "import re\n",
    "import commonUtils\n",
    "import constants\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boss',\n",
       " 'captain',\n",
       " 'champion',\n",
       " 'executive',\n",
       " 'manager',\n",
       " 'refree',\n",
       " 'said',\n",
       " 'skipper'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.positivePrefixSuffixList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCleanedWordListFromFile(fileName):\n",
    "    wordlist = []\n",
    "    fileText = open(fileName).read()\n",
    "    for line in fileText.splitlines():\n",
    "        for pattern in constants.nullReplaceList:\n",
    "            line = line.replace(pattern,'')\n",
    "        for pattern in constants.spaceReplaceList:\n",
    "            line = line.replace(pattern,' ')\n",
    "        for word in line.split(' '):\n",
    "            word = word.strip('\\'')\n",
    "            word = word.replace(\"[[[\",\"[[\")\n",
    "            word = word.replace(\"]]]\",\"]]\")\n",
    "            if(len(word)>=1):\n",
    "                wordlist.append(word)\n",
    "    return wordlist\n",
    "\n",
    "def getStringCombinationsFromWordList(wordlist):\n",
    "    allPossibeStringCombinations = []\n",
    "    for i in range(0,len(wordlist)):\n",
    "        currString = \"\"\n",
    "        for j in range(0,3):\n",
    "            if(i+j>=len(wordlist)):\n",
    "                break\n",
    "            if wordlist[i+j].lower() in [ig.lower() for ig in constants.wordsToIgnoreList]:\n",
    "                break\n",
    "            if any(char.isdigit() for char in wordlist[i+j]) :\n",
    "                break\n",
    "            if j > 0:\n",
    "                currString+=' '\n",
    "            currString+= wordlist[i+j]\n",
    "            if len(currString.strip(' ')) >=1:\n",
    "                allPossibeStringCombinations.append([currString.strip(' '),i,i+j])\n",
    "    return allPossibeStringCombinations\n",
    "\n",
    "\n",
    "def getPositivesAndNegatives(allPossibeStringCombinations):\n",
    "    positive = []\n",
    "    negative = []\n",
    "    for s,i,j in allPossibeStringCombinations:\n",
    "        if s.startswith('[[') and s.endswith(']]'):\n",
    "            if (\"[\" not in s[2:-2]) and (\"]\" not in s[2:-2]):\n",
    "                positive.append([s,i,j])\n",
    "            else:\n",
    "                negative.append([s,i,j])\n",
    "        else:\n",
    "            negative.append([s,i,j])\n",
    "    return positive,negative\n",
    "\n",
    "def getFeature1FirstWordCapital(token):\n",
    "    #[word, start, end]\n",
    "    # checks if first word of every word in token is capital\n",
    "    feature = 1\n",
    "    for word in token[0].split():\n",
    "        word = word.replace(\"[[\", '')\n",
    "        word = word.replace(\"]]\", '')\n",
    "        feature = feature & word[0].isupper()\n",
    "    return feature\n",
    "\n",
    "def getFeature2PrefixWordCapital(token, wordList):\n",
    "    #check if words either prev or after have capital letters\n",
    "    # flaky - \"Tom Cruise does\" - false positive for 'does'\n",
    "    # maybe helps to learn something\n",
    "    feature = 0\n",
    "    if token[1] > 0:\n",
    "        cmpWord = wordList[token[1] - 1].replace(\"[[\", '')\n",
    "        cmpWord = cmpWord.replace(\"]]\", '')\n",
    "        feature |= cmpWord[0].isupper()\n",
    "        \n",
    "    if token[2] < (len(wordList) - 1):\n",
    "        cmpWord = wordList[token[2] + 1].replace(\"[[\", '')\n",
    "        cmpWord = cmpWord.replace(\"]]\", '')\n",
    "        feature |= cmpWord[0].isupper()\n",
    "    return feature\n",
    "\n",
    "def getFeature7SuffixWordCapital(token, wordList):\n",
    "    #check if words either prev or after have capital letters\n",
    "    # flaky - \"Tom Cruise does\" - false positive for 'does'\n",
    "    # maybe helps to learn something\n",
    "    feature = 0\n",
    "        \n",
    "    if token[2] < (len(wordList) - 1):\n",
    "        cmpWord = wordList[token[2] + 1].replace(\"[[\", '')\n",
    "        cmpWord = cmpWord.replace(\"]]\", '')\n",
    "        feature |= cmpWord[0].isupper()\n",
    "    return feature\n",
    "\n",
    "def getFeature8Prefix2WordCapital(token, wordList):\n",
    "    #check if words either prev or after have capital letters\n",
    "    # flaky - \"Tom Cruise does\" - false positive for 'does'\n",
    "    # maybe helps to learn something\n",
    "    feature = 0\n",
    "    if token[1] > 1:\n",
    "        cmpWord = wordList[token[1] - 2].replace(\"[[\", '')\n",
    "        cmpWord = cmpWord.replace(\"]]\", '')\n",
    "        feature |= cmpWord[0].isupper()\n",
    "\n",
    "    return feature\n",
    "\n",
    "def getFeature9Suffix2WordCapital(token, wordList):\n",
    "    #check if words either prev or after have capital letters\n",
    "    # flaky - \"Tom Cruise does\" - false positive for 'does'\n",
    "    # maybe helps to learn something\n",
    "    feature = 0\n",
    "        \n",
    "    if token[2] < (len(wordList) - 2):\n",
    "        cmpWord = wordList[token[2] + 2].replace(\"[[\", '')\n",
    "        cmpWord = cmpWord.replace(\"]]\", '')\n",
    "        feature |= cmpWord[0].isupper()\n",
    "    return feature\n",
    "\n",
    "def getFeature3TokenLength(token):\n",
    "    return len(token[0].split())\n",
    "\n",
    "def getFeature4ProbPreSuffix(token, wordList):\n",
    "    feature = 0\n",
    "    if token[1] > 0:\n",
    "        cmpWord = wordList[token[1] - 1].replace(\"[[\", '')\n",
    "        cmpWord = cmpWord.replace(\"]]\", '')\n",
    "        if cmpWord.lower() in [ig.lower() for ig in constants.positivePrefixSuffixList]:\n",
    "            feature |= 1\n",
    "    \n",
    "    if token[2] < (len(wordList) - 1):\n",
    "        cmpWord = wordList[token[2] + 1].replace(\"[[\", '')\n",
    "        cmpWord = cmpWord.replace(\"]]\", '')\n",
    "        if cmpWord.lower() in [ig.lower() for ig in constants.positivePrefixSuffixList]:\n",
    "            feature |= 1\n",
    "    return feature\n",
    "\n",
    "def getFeature10PrefixWordLength(token, wordList):\n",
    "    #check if words either prev or after have capital letters\n",
    "    # flaky - \"Tom Cruise does\" - false positive for 'does'\n",
    "    # maybe helps to learn something\n",
    "    feature = 0\n",
    "    if token[1] > 0:\n",
    "        cmpWord = wordList[token[1] - 1].replace(\"[[\", '')\n",
    "        cmpWord = cmpWord.replace(\"]]\", '')\n",
    "        feature = len(cmpWord)\n",
    "        \n",
    "    return feature\n",
    "\n",
    "def getFeature11SuffixWordLength(token, wordList):\n",
    "    #check if words either prev or after have capital letters\n",
    "    # flaky - \"Tom Cruise does\" - false positive for 'does'\n",
    "    # maybe helps to learn something\n",
    "    feature = 0\n",
    "        \n",
    "    if token[2] < (len(wordList) - 1):\n",
    "        cmpWord = wordList[token[2] + 1].replace(\"[[\", '')\n",
    "        cmpWord = cmpWord.replace(\"]]\", '')\n",
    "        feature = len(cmpWord)\n",
    "    return feature\n",
    "\n",
    "#check if want to normalize it in some way\n",
    "def getFeature5TokenHash(token):\n",
    "    #http://cseweb.ucsd.edu/~kube/cls/100/Lectures/lec16/lec16-16.html\n",
    "    hashVal = 0\n",
    "    word = token[0].replace(\"[[\", '')\n",
    "    word = word.replace(\"]]\", '')\n",
    "    for char in word:\n",
    "        hashVal = (hashVal << 4) + ord(char)\n",
    "        g = hashVal & 0xF0000000\n",
    "        if g != 0:\n",
    "            hashVal = hashVal ^ (g >> 24)\n",
    "        hashVal = hashVal & ~g\n",
    "    return hashVal\n",
    "\n",
    "def getFeature6OneHotVector(token):\n",
    "    #separate 26-26 for caps and lower case\n",
    "    charDictCaps = {chr(i) : 0 for i in range(65,91)}\n",
    "    charDictSmall = {chr(i) : 0 for i in range(97, 123)}\n",
    "    for char in token[0]:\n",
    "        if char in charDictCaps:\n",
    "            charDictCaps[char] += 1\n",
    "        elif char in charDictSmall:\n",
    "            charDictSmall[char] += 1\n",
    "    \n",
    "    charIdxCaps = {key : i for i,key in enumerate(charDictCaps.keys())}\n",
    "    charIdxSmall = {key : (i+26) for i,key in enumerate(charDictSmall.keys())}\n",
    "\n",
    "    OHvector = np.zeros((1,52))\n",
    "    for key in charDictCaps.keys():\n",
    "        OHvector[0,charIdxCaps[key]] = charDictCaps[key]\n",
    "    for key in charDictSmall.keys():\n",
    "        OHvector[0,charIdxSmall[key]] = charDictSmall[key]\n",
    "    \n",
    "    return OHvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3950 81967\n"
     ]
    }
   ],
   "source": [
    "# pre-split data check\n",
    "folderNames = ['Abhinav','Bidyut','Chirayu']\n",
    "folderPath = '../dataset_markup/'\n",
    "totalMarkups = 0\n",
    "totalUniqueMarkups = set()\n",
    "p_total =0\n",
    "n_total = 0\n",
    "for fileName in commonUtils.getAllFiles(folderNames,folderPath):\n",
    "    wordList = getCleanedWordListFromFile(fileName)\n",
    "    l = getStringCombinationsFromWordList(wordList)\n",
    "    p,n = getPositivesAndNegatives(l)\n",
    "    p_total+=len(p)\n",
    "    n_total+=len(n)\n",
    "    \n",
    "print(p_total,n_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #randomly separate 200 train and 100 test files\n",
    "# folderNames = ['documentPool']\n",
    "# folderPath = '../stage1/'\n",
    "# fileList = commonUtils.getAllFiles(folderNames,folderPath)\n",
    "\n",
    "# import random\n",
    "# import shutil\n",
    "# #twice random only to introduce more randomness\n",
    "# corpusIdx = [i for i in range(0,len(fileList))]\n",
    "# testPool = random.sample(corpusIdx, 100)\n",
    "# testPool = random.sample(corpusIdx, 100)\n",
    "# trainPool = set(corpusIdx).symmetric_difference(set(testPool))\n",
    "\n",
    "# for idx in trainPool:\n",
    "#     shutil.copy(fileList[idx], '../stage1/train')\n",
    "    \n",
    "# for idx in testPool:\n",
    "#     shutil.copy(fileList[idx], '../stage1/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2719 28297\n",
      "(31016, 63)\n",
      "1217 13574\n",
      "(14791, 63)\n"
     ]
    }
   ],
   "source": [
    "#create train, test db\n",
    "arrColumns = 58 + 1 + 2 + 2\n",
    "\n",
    "def getFeatureVector(token, wordList):\n",
    "    featureVector = np.zeros((1,arrColumns))\n",
    "    featureVector[0,0] = getFeature1FirstWordCapital(token)\n",
    "    featureVector[0,1] = getFeature2PrefixWordCapital(token, wordList)\n",
    "    featureVector[0,2] = getFeature7SuffixWordCapital(token, wordList)\n",
    "    featureVector[0,3] = getFeature8Prefix2WordCapital(token, wordList)\n",
    "    featureVector[0,4] = getFeature9Suffix2WordCapital(token, wordList)\n",
    "    featureVector[0,5] = getFeature3TokenLength(token)\n",
    "    featureVector[0,6] = getFeature10PrefixWordLength(token, wordList)\n",
    "    featureVector[0,7] = getFeature11SuffixWordLength(token, wordList)\n",
    "    \n",
    "    featureVector[0,8] = getFeature4ProbPreSuffix(token, wordList)\n",
    "    featureVector[0,9] = getFeature5TokenHash(token)\n",
    "    \n",
    "    \n",
    "    featureVector[0,10:62] = getFeature6OneHotVector(token)\n",
    "    return featureVector\n",
    "\n",
    "def getMetaData(token, wordList, fileName, l):\n",
    "    meta = []\n",
    "    meta.append(token)\n",
    "    w = ''\n",
    "    if token[1] > 0:\n",
    "        w = wordList[token[1] - 1]\n",
    "    meta.append(w)\n",
    "    w = ''\n",
    "    if (token[2] < (len(wordList) - 1)):\n",
    "        w = wordList[token[2] + 1]\n",
    "    meta.append(w)\n",
    "    meta.append(fileName)\n",
    "    meta.append(l)\n",
    "    return meta\n",
    "            \n",
    "folderNames = ['train', 'test']\n",
    "folderPath = '../stage1/'\n",
    "for folderName in folderNames:\n",
    "    temp = [folderName]\n",
    "    fileList = commonUtils.getAllFiles(temp,folderPath)\n",
    "    p_total = 0\n",
    "    n_total = 0\n",
    "\n",
    "    # token, prev, after, fileName\n",
    "    metaData = []\n",
    "    trainArray = np.zeros((1000,arrColumns))\n",
    "    arrayIndex = 0\n",
    "\n",
    "    for fileName in fileList:\n",
    "        wordList = getCleanedWordListFromFile(fileName)\n",
    "        l = getStringCombinationsFromWordList(wordList)\n",
    "        p,n = getPositivesAndNegatives(l)\n",
    "\n",
    "        for data in p:\n",
    "            meta = []\n",
    "            p_total += 1\n",
    "            # 1 for pos\n",
    "            featureVector = getFeatureVector(data, wordList)\n",
    "            featureVector[0,-1] = 1\n",
    "            if arrayIndex > (trainArray.shape[0] - 1):\n",
    "                trainArray.resize((trainArray.shape[0]+1000, arrColumns))\n",
    "\n",
    "            trainArray[arrayIndex, :] = featureVector\n",
    "            arrayIndex += 1\n",
    "            metaData.append(getMetaData(data, wordList, fileName, 1))\n",
    "\n",
    "        for data in n:\n",
    "            # randomly drop some negative samples\n",
    "            if True:#random.choice([True, False]) or folderName == 'test':\n",
    "                n_total += 1\n",
    "                featureVector = getFeatureVector(data, wordList)\n",
    "                # 0 for negative\n",
    "                featureVector[0,-1] = 0\n",
    "                if arrayIndex > (trainArray.shape[0] - 1):\n",
    "                    trainArray.resize((trainArray.shape[0]+1000, arrColumns))\n",
    "\n",
    "                trainArray[arrayIndex, :] = featureVector\n",
    "                arrayIndex += 1\n",
    "                metaData.append(getMetaData(data, wordList, fileName, 0))\n",
    "\n",
    "    trainArray = trainArray[0:p_total+n_total, :]\n",
    "    sfl = np.arange(trainArray.shape[0]) \n",
    "    np.random.shuffle(sfl)\n",
    "    trainArray = trainArray[sfl]\n",
    "    metaSfl = []\n",
    "    for idx in sfl:\n",
    "        metaSfl.append(metaData[idx])\n",
    "\n",
    "    print(p_total,n_total)\n",
    "    print (trainArray.shape)\n",
    "\n",
    "    npArrFile = folderName + '.npy'\n",
    "    np.save(os.path.join(folderPath, npArrFile), trainArray)\n",
    "    \n",
    "    #write meta data to text file\n",
    "    metaFile = folderName + 'MetaData.txt'\n",
    "    fp = open(os.path.join(folderPath, metaFile), 'w')\n",
    "    for line in metaSfl:\n",
    "        fp.write(str(line))\n",
    "        fp.write('\\n')\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
